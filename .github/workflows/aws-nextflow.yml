# naming the workflow so that it will show up in github actions
name: Run Nextflow Pipeline on AWS

# "on" defines when this workflow should run
# "workflow_dispatch" means you manually trigger it by clicking a button on github's website
# this is different from automatic triggers like "push" or "pull_request"
on:
  workflow_dispatch:
    # input boxes that appear when you click "run workflow" on github
    # this let you customize settings each time you run the pipeline without editing code. woohoo!
    inputs:
      # first input: where your DNA sequencing files are stored in aws s3
      input_s3_path:
        description: 'S3 path to input FASTQ files'  # text the user sees in the input box
        required: true                                # user must fill this in - can't be empty
        default: 's3://bp-wgs-covaris-input-data/samples'  # your actual input bucket
      
      # second input: where you want the results to be saved in aws s3
      output_s3_path:
        description: 'S3 path for results'
        required: true
        default: 's3://bp-wgs-covaris-nextflow-results/results'
      
      # third input: which aws region (geographic location) to use
      # use the same region where you deployed your cloudformation stack
      aws_region:
        description: 'AWS region (e.g., us-east-1, us-east-2, eu-west-1)'
        required: true
      
      # fourth input: which version of nextflow software to use
      nextflow_version:
        description: 'Nextflow version to use'
        required: false
        default: '23.10.1'

# jobs are the actual work that gets done
# you can have multiple jobs running in parallel, but we only need one
jobs:
  run-nextflow:
    # this tells github to use an ubuntu linux virtual computer to run our commands
    # github provides this for free and it comes with lots of software pre-installed
    runs-on: ubuntu-latest
    
    # these are special permissions github needs to securely connect to aws
    permissions:
      id-token: write   # needed to create a temporary token to prove identity to aws
      contents: read    # needed to download your code from the github repo
    
    # steps are the individual tasks that run in order
    # each step runs completely before the next one starts
    steps:
      # step 1: download your code from github to the virtual computer
      - name: Checkout repository
        uses: actions/checkout@v4  # this is a pre-made action created by github
      
      # step 2: connect to aws using your stored credentials
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4  # pre-made action by amazon
        with:
          # this gets the aws role arn from your github secrets
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          # this uses whichever region you typed in when running the workflow
          aws-region: ${{ github.event.inputs.aws_region }}
      
      # step 3: install java programming language if not installed
      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'corretto'  # amazon's free version of java
          java-version: '17'        # nextflow requires java 17 or later
      
      # step 4: download and install nextflow software
      - name: Install Nextflow
        # "run:" means execute these shell commands directly
        run: |
          # download the nextflow installer from the internet and run it
          wget -qO- https://get.nextflow.io | bash -s -- -version ${{ github.event.inputs.nextflow_version }}
          # move the nextflow program to a standard location where linux can find it
          sudo mv nextflow /usr/local/bin/
          # make the file executable (give permission to run it)
          chmod +x /usr/local/bin/nextflow
      
      # step 5: test that aws connection is working correctly before starting the job
      - name: Validate AWS setup
        run: |
          # check what aws account we're connected to
          aws sts get-caller-identity
          # check that our batch compute environment exists and is ready
          aws batch describe-compute-environments --region ${{ github.event.inputs.aws_region }}
      
      # step 5b: debug s3 contents to see what files are actually there
      - name: Debug S3 input files
        run: |
          echo "Listing contents of input S3 bucket:"
          aws s3 ls ${{ github.event.inputs.input_s3_path }}/ --recursive
          echo ""
          echo "Looking specifically for fastq.gz files:"
          aws s3 ls ${{ github.event.inputs.input_s3_path }}/ --recursive | grep "\.fastq\.gz"
          echo ""
          echo "Total file count:"
          aws s3 ls ${{ github.event.inputs.input_s3_path }}/ --recursive | wc -l
      
      # step 6: debug what parameters we're about to pass
      - name: Debug Nextflow parameters
        run: |
          echo "Input S3 path: '${{ github.event.inputs.input_s3_path }}'"
          echo "Output S3 path: '${{ github.event.inputs.output_s3_path }}'"
          echo "AWS region: '${{ github.event.inputs.aws_region }}'"
          echo "Nextflow version: '${{ github.event.inputs.nextflow_version }}'"
      
      # step 7: test s3 access with a simple nextflow script first
      - name: Test S3 Access from Nextflow/Batch
        run: |
          # create a simple test script to check s3 access from batch environment
          cat > s3_test.nf << 'EOF'
          #!/usr/bin/env nextflow
          nextflow.enable.dsl = 2
          
          process TEST_S3_ACCESS {
            executor 'awsbatch'
            queue 'nextflow-compute-queue'
            container 'amazonlinux:2'
            
            output:
            stdout
            
            script:
            """
            echo "=== AWS/S3 Access Test from Batch Container ==="
            echo "Current user: \$(whoami)"
            echo "Current directory: \$(pwd)"
            echo ""
            
            echo "Testing AWS CLI availability:"
            which aws || echo "AWS CLI not found"
            aws --version || echo "AWS CLI version failed"
            echo ""
            
            echo "Testing AWS credentials:"
            aws sts get-caller-identity || echo "AWS credentials failed"
            echo ""
            
            echo "Testing S3 access:"
            aws s3 ls s3://bp-wgs-covaris-input-data/ || echo "S3 bucket listing failed"
            echo ""
            
            echo "Testing specific file listing:"
            aws s3 ls s3://bp-wgs-covaris-input-data/samples/ || echo "S3 samples folder listing failed"
            echo ""
            
            echo "Testing file pattern with AWS CLI:"
            aws s3 ls s3://bp-wgs-covaris-input-data/samples/ | grep "\.fastq\.gz" || echo "No fastq.gz files found"
            echo ""
            
            echo "=== End S3 Access Test ==="
            """
          workflow {
            TEST_S3_ACCESS() | view
          }
          EOF
          
          echo "Running S3 access test:"
          nextflow run s3_test.nf -profile aws
      
      # step 8: actually run our bioinformatics pipeline
      - name: Run Nextflow pipeline
        run: |
          # show the exact command we're about to run
          echo "About to run this Nextflow command:"
          echo "nextflow run main.nf -profile aws --input_dir \"${{ github.event.inputs.input_s3_path }}\" --outdir \"${{ github.event.inputs.output_s3_path }}\" --aws_region \"${{ github.event.inputs.aws_region }}\" -with-report nextflow_report.html -with-timeline timeline.html -with-dag flowchart.html"
          echo ""
          
          # run the main.nf file (pipeline) with these settings:
          nextflow run main.nf \
            -profile aws \
            --input_dir "${{ github.event.inputs.input_s3_path }}" \
            --outdir "${{ github.event.inputs.output_s3_path }}" \
            --aws_region "${{ github.event.inputs.aws_region }}" \
            -with-report nextflow_report.html \
            -with-timeline timeline.html \
            -with-dag flowchart.html
      
      # step 7: save the reports so you can download them from github later
      - name: Upload Nextflow reports
        if: always()  # do this step even if the pipeline failed for debugging
        uses: actions/upload-artifact@v4
        with:
          name: nextflow-reports  # what to call the download package
          # specific files to save:
          path: |
            nextflow_report.html   # summary of resource usage, success/failure
            timeline.html          # when each process started and finished
            flowchart.html         # visual diagram of your pipeline
            .nextflow.log          # detailed log file for troubleshooting
      
      # step 8: clean up temporary files to avoid using too much storage space
      - name: Cleanup
        if: always()  # do this no matter what happened in previous steps
        run: |
          # remove temporary nextflow working files
          # these can be large and we don't need them since results are in s3
          rm -rf work/ .nextflow*